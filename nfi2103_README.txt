README file
nfi2103
Project 4
C++

The programming language I implemented Project 4 in is C++. As for development environment, I mostly worked on XCode 4.6. I also tested my code with the Mac terminal compiler. I couldn't find out what working directory XCode uses on my laptop, so when using XCode I had to specify the full path of the files I wanted to read from and write to. Lines 69 and 345, which are commented out in my final code, were used to run the code on XCode. When compiling on the Mac terminal, I didn't have any issues with setting up the correct working directory so these lines were simply commented out and filename is appropriately provided by user input.

To run and compile using Mac Terminal, I simply used ls and cd commands to go to the directory where file nfi2103.cpp is stored. Then, I used the commands "make nfi2103" and "./nfi2103" to compile and run the code on the terminal. The program asks for the name of the training file data (which has to be in the same working directory as nfi2103.cpp) and then outputs a file called "nfi2103-decisionTree.cpp". To run this file on test data, simply use the commands "make nfi2103-decisionTree" and "./nfi2103-decisionTree". The program asks for the name of the training file data (which has to be in the same working directory where nfi2103.cpp and nfi2103-decisionTree.cpp are) and then outputs the appropriate label according to the decision Tree nfi2103.cpp constructed with basis on the training data.

File nfi2103_test.txt has the terminal output of nfi2103-decisionTree.cpp. This test file is not a .cpp file but rather a .txt file because nfi2103.cpp is compiled on the command line (either in Xcode or in the Mac terminal) and its output is already a .cpp file (namely, nfi2103-decisionTree.cpp)

Details of how the program works are specified in comments throughout the code. Basically, I implemented the DECISION-TREE LEARNING algorithm whose pseudo-code is provided in Chapter 18, Figure 18.5 of the textbook (page 702 in my book). I made an effort to abide by the pseudo-code as much as possible. The main difference is that the textbook pseudo-code has DECISION-TREE LEARNING return different data types in different situations (it returns a tree in the general case, but it returns a number when 'examples' or 'attributes' is empty and returns a string 'classification' when all examples have the same classification). To cimcurvent the problem of having the same function return different data types, I implemented DECISION-TREE LEARNING with void return, but with arguments passed by reference (so we initially pass an empty tree as argument and then augment it with subtrees in each recursive call). IMPORTANCE and PLURALITY-VALUE are implemented according to the textbook specification as well (that is, based on functions ENTROPY, REMAINDER and GAIN, which are all described in the textbook).

The function commented out at the bottom of the code (AssignLabels plus its supporting function PrintExample Row) does the same job as output .cpp file nfi2103-decisionTree.cpp, that is, prints the classification labels of examples in a test data set. The function is commented out because generating a program to label examples in test datasets was a requirement in this assignment.

